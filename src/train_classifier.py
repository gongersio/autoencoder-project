import pickle
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, ConcatDataset, DataLoader

from models import Autoencoder, Classifier
from load_dataset import FrameDataset
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

def load_dataset(file_path):
    '''Load a previously saved dataset from the specified file path.'''
    with open(file_path, 'rb') as f:
        dataset = pickle.load(f)

    return dataset

def load_encoder(file_path):
    '''Load a pre-trained autoencoder from the specified file path.'''
    autoencoder = Autoencoder()
    autoencoder.load_state_dict(torch.load(file_path))
    autoencoder.eval()
    
    #Extract and return only the encoder part of the model.
    return autoencoder.encoder

def extract_features(encoder, dataloader):
    '''Extract features from the latent representation generated by the encoder.'''
    features = []
    labels = []

    with torch.no_grad():
        for batch_frames, batch_labels in dataloader:
            batch_features = encoder(batch_frames)

            #Flatten the features from a multi-dimensional tensor to a single dimension vector.
            features.append(batch_features.view(batch_features.size(0), -1))
            labels.append(batch_labels)
    
    #Concatenate all the extracted features and labels into tensors.
    features = torch.cat(features)
    labels = torch.cat(labels)

    return features, labels

def create_feature_dataset(encoder: Autoencoder, frame_dataset: FrameDataset, shuffle=True):
    '''Creates a feature dataset from a given frame dataset.'''
    frame_loader = DataLoader(frame_dataset, batch_size=32, shuffle=shuffle, num_workers=4, pin_memory=True)
    features, labels = extract_features(encoder, frame_loader)

    feature_dataset = TensorDataset(features, labels)
    feature_loader = DataLoader(feature_dataset, batch_size=32, shuffle=shuffle, num_workers=4, pin_memory=True)

    return features, feature_loader

def train_classifier(train_loader, eval_loader, input_dim, hidden_dim, output_dim, num_epochs=20):
    '''Train the classifier model using the extracted features and labels, for the specified number of epochs.'''
    #Initialise the model, criterion, and optimiser.
    model = Classifier(input_dim, hidden_dim, output_dim)
    criterion = nn.CrossEntropyLoss()
    optimiser = optim.Adam(model.parameters(), lr=0.001)

    for epoch in range(num_epochs):
        model.train() #Set the model to training mode.
        total_train_loss = 0.0
        
        for batch_features, batch_labels in train_loader:
            #Clear all the gradients.
            optimiser.zero_grad()

            #Perform a forward pass of the model on the features.
            predictions = model(batch_features)

            #Calculate the loss between the predictions and the actual label.
            batch_loss = criterion(predictions, batch_labels)

            #Perform a backward pass and update the model weights based on the loss.
            batch_loss.backward()
            optimiser.step()

            #Calculate the total epoch loss.
            total_train_loss += batch_loss.item() * batch_features.size(0)
        
        #Calculate the average training loss per epoch.
        avg_train_loss = total_train_loss / len(train_loader.dataset)

        model.eval() #Set the model to evaluation mode.
        total_eval_loss = 0.0
        correct = 0
        total = 0

        with torch.no_grad():
            for batch_features, batch_labels in eval_loader:
                predictions = model(batch_features)
                batch_loss: torch.Tensor = criterion(predictions, batch_labels)
                total_eval_loss += batch_loss.item() * batch_features.size(0)

                #Get the all the predicted class labels and count the number of correct predictions. 
                _, predicted = torch.max(predictions, 1) 
                total += batch_labels.size(0)
                correct += (predicted == batch_labels).sum().item()

        #Calculate the average validation loss per epoch and the validation accuracy.
        avg_eval_loss = total_eval_loss / len(eval_loader.dataset)
        eval_accuracy = correct/total    
        
        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_eval_loss:.4f}, Val Accuracy: {eval_accuracy:.4f}')

    return model

def evaluate_classifier(model: Classifier, test_features, test_labels):
    '''Evaluates the performance of the trained classifier model on a test dataset.'''
    model.eval() #Set  the model to evaluation mode.

    with torch.no_grad():
        predictions = model(test_features)
        _, predicted = torch.max(predictions, 1)

        accuracy = accuracy_score(test_labels, predicted)
        precision = precision_score(test_labels, predicted, average='binary')
        recall = recall_score(test_labels, predicted, average='binary')
        f1 = f1_score(test_labels, predicted, average='binary')
    
    print(f'Accuracy: {accuracy:.4f}')
    print(f'Precision: {precision:.4f}')
    print(f'Recall: {recall:.4f}')
    print(f'F1 Score: {f1:.4f}')

if __name__ == "__main__":
    encoder = load_encoder("autoencoder.pth")
    
    train_dataset = load_dataset("train_classifier.pkl")
    train_features, train_loader = create_feature_dataset(encoder, train_dataset)

    eval_dataset = load_dataset("eval_classifier.pkl")
    eval_features, eval_loader = create_feature_dataset(encoder, eval_dataset, shuffle=False)

    test_dataset = load_dataset("test_classifier.pkl")
    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)
    test_features, test_labels = extract_features(encoder, test_loader)

    classifier = train_classifier(train_loader, eval_loader, train_features.size(1), 128, 2)
    evaluate_classifier(classifier, test_features, test_labels)

    #Save the classifier model.
    torch.save(classifier.state_dict(), "classifier.pth")
    print("Model saved.")
